{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac65c355-483a-4714-9802-f56a5c35d986",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor:\n",
    "    def __init__(self, eta, runs):\n",
    "        self.eta = eta\n",
    "        self.runs = runs\n",
    "        self.W = None\n",
    "\n",
    "    def fit(self, x, y, w_init):\n",
    "        \"\"\"\n",
    "        Optimize the weights W to minimize the negative log-likelihood by using gradient descent\n",
    "\n",
    "        :param x: a 2D numpy array of transformed feature values. Shape is (n x 2), (n x 3), or (n x 6)\n",
    "        :param y: a 2D numpy array of output values. Shape is (n x 1)\n",
    "        :param w_init: a 2D numpy array that initializes the weights. Shape is (2 x 1), (3 x 1), or (6 x 1)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Keep this for the autograder\n",
    "        self.W = w_init\n",
    "        # TODO\n",
    "        N = x.shape[0]\n",
    "        for run in range(self.runs):\n",
    "            # Compute gradient\n",
    "            # grad = np.zeros(self.W.shape)\n",
    "            # assert grad.shape == self.W.shape\n",
    "            grad = np.dot((self.predict(x) - y).T, x).T / N\n",
    "            # Iterate over all points in data set\n",
    "            # for x_n, y_n in zip(x, y):\n",
    "            #     add = -(y_n[0] - sigmoid(np.dot(self.W.T, x_n)[0])) * x_n\n",
    "            #     grad += np.reshape(add, (-1, 1))\n",
    "            # Update weights W in direction of negative gradient\n",
    "            self.W = self.W - self.eta * grad\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict classification probability of transformed input x\n",
    "        \n",
    "        :param x: a 2D numpy array of transformed feature values. Shape is (n x 2), (n x 3), or (n x 6)\n",
    "        :return: a 2D numpy array of predicted probabilities given current weights. Shape should be (n x 1)\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return sigmoid(np.dot(self.W, x))\n",
    "        # return np.array([[sigmoid(np.dot(self.W.T, x_n)[0])] for x_n in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12952ac3-5506-4032-b7e1-542491b6cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a one-hot vector from class value\n",
    "def one_hot(class_val, num_classes):\n",
    "    y = np.zeros(num_classes, dtype=int)\n",
    "    y[class_val] = 1\n",
    "    return y\n",
    "\n",
    "# Softmax classifier with L_2 regularization\n",
    "class SoftmaxRegression:\n",
    "    def __init__(self, eta, lam):\n",
    "        self.eta = eta\n",
    "        self.lam = lam\n",
    "        self.W = None\n",
    "        self.runs = 200000\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the weights W of softmax regression using gradient descent with L2 regularization\n",
    "        in the form (lambda/2) * norm(w)^2\n",
    "        Use the results from Problem 2 to find an expression for the gradient\n",
    "        \n",
    "        :param X: a 2D numpy array of (transformed) feature values. Shape is (n x 2)\n",
    "        :param y: a 1D numpy array of target values (Dwarf=0, Giant=1, Supergiant=2).\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        # Add bias column to features X\n",
    "        X = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "        # Initializing the weights (do not change!)\n",
    "        # The number of classes is 1 + (the highest numbered class)\n",
    "        num_classes = 1 + y.max()\n",
    "        num_features = X.shape[1]\n",
    "        self.W = np.ones((num_classes, num_features))\n",
    "        # Convert y into array of one-hot vectors\n",
    "        y = np.array([one_hot(y_n, num_classes) for y_n in y])\n",
    "        # Iterate over each row of weights\n",
    "        for run in range(self.runs):\n",
    "            preds = self.predict_proba(X)\n",
    "            for j in range(num_classes):\n",
    "                # Compute gradient\n",
    "                # grad_wj = self.lam * self.W[j]\n",
    "                # for x_n, y_n in zip(X, y):\n",
    "                # # Add to gradient of NLL\n",
    "                #     grad_wj += (softmax(np.dot(self.W, x_n))[j] - y_n[j]) * x_n\n",
    "                grad_wj = np.dot(preds[:, j] - y[:, j], X)\n",
    "                grad_wj += self.lam * self.W[j]\n",
    "                # Update the row of weights\n",
    "                self.W[j] -= self.eta * grad_wj\n",
    "\n",
    "\n",
    "    def predict(self, X_pred):\n",
    "        \"\"\"\n",
    "        The code in this method should be removed and replaced! We included it\n",
    "        just so that the distribution code is runnable and produces a\n",
    "        (currently meaningless) visualization.\n",
    "        \n",
    "        Predict classes of points given feature values in X_pred\n",
    "        \n",
    "        :param X_pred: a 2D numpy array of (transformed) feature values. Shape is (n x 2)\n",
    "        :return: a 1D numpy array of predicted classes (Dwarf=0, Giant=1, Supergiant=2).\n",
    "                 Shape should be (n,)\n",
    "        \"\"\"\n",
    "        # Add column of ones at the beginning of X_pred matrix\n",
    "        # X_pred = np.hstack([np.ones((X_pred.shape[0], 1)), X_pred])\n",
    "        return np.array([softmax(np.dot(self.W, x)).argmax() for x in X_pred])\n",
    "    \n",
    "    def predict_proba(self, X_pred):\n",
    "        \"\"\"    \n",
    "        Predict classification probabilities of points given feature values in X_pred\n",
    "        \n",
    "        :param X_pred: a 2D numpy array of (transformed) feature values. Shape is (n x 2)\n",
    "        :return: a 2D numpy array of predicted class probabilities (Dwarf=index 0, Giant=index 1, Supergiant=index 2).\n",
    "                 Shape should be (n x 3)\n",
    "        \"\"\"\n",
    "        # Add column of ones at the beginning of X_pred matrix\n",
    "        # X_pred = np.hstack([np.ones((X_pred.shape[0], 1)), X_pred])\n",
    "        return np.array([softmax(np.dot(self.W, x)) for x in X_pred])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
