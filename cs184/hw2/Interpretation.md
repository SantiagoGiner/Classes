Overall, if we just look at the average cumulative regret at the very last time step for every algorithm, we can see that Thompson sampling has the lowest such value, followed by ETC, UCB,  $\varepsilon$-Greedy, Greedy, and lastly, Explore, which reaches the highest mean cumulative regret at the last time step. Simply looking at this number is not enough to speak of the overall performance of each algorithm, however, for we must also look at how such reward is accumulated with time as well as how it varies accross different runs, which is given by the width of the shaded region showing the range between the 5th and 95th percentiles.

- Explore: this has very low variation in the regret, as conveyed by the very small width of the shaded region. This makes sense since we are choosing random arms at every pull, so we are never actually using any information from the rewards we get. Thus, our regret should always grow in a very similar way, since there is essentially nothing that will differentiate one run from another in a significant way.
- Greedy: we see a really large variation in the mean cumulative regret. This makes sense for the following reason. Given that each arm's reward is random, exploring only once means that the choice of best arm will be widely varied, since each arm has a nonzero probability of giving a 1. Thus, since we break ties uniformly at random, it makes sense that we see such a wide variation in the regret, because we are essentially, though not exactly, choosing random arms to exploit every time, and so our regret should be very different every time. Note also that, once we choose to exploit one arm, the regret should grow as a straight line, which we indeed see in the plot.
- ETC: here we see a balance between the last two cases. That is, initially, we explore all arms at random, which explains the small variation in the regret accumulation for the first 50 time steps. Then there is a sudden change in the regret accumulation, for we transition into the exploitation phase. The remaining growth then looks very similar to that of the Greedy case: an increasing line with constant slope that has large variation. The large variation in this second phase comes from the stochastic nature of the rewards, which will mean that the arm with the highest empirical mean at the end of the exploration phase will vary despite there being an actual underlying best arm that carries the highest true reward mean.
- $\varepsilon$-Greedy, UCB, and Thompson sampling: the three of these have variations in the regret that lie somewhere in the middle between the large variation of Greedy and the minimal variation of Explore. $\varepsilon$-Greedy and UCB have lower variation than Thompson sampling, however, which seems to be largely varied, but its 95th percentile still reaches lower final regret values than all other algorithms.

Taking these considerations into account, one would be be justified in saying that Explore and Greedy are, at least in comparison with the other algorithms, "bad," for Explore accumulates too large a regret while Greedy is so widely varied than one would never get a consistent output when deploying it in practice. For the remaining four algorithms, the situation is not as clear, however, as the choice of "better" algorithm will depend on the tradeoff that exists between being greedy, i.e. exploiting the arm that we currently see is best, and exploring, i.e. finding more information about arms we have not interacted with, as well as our knowledge of the MAB's parameters. Both UCB and $\varepsilon$-Greedy seem to be the algorithms that find the best balance between the two, in the sense that they do not incur very large cumulative regrets while at the same time being more consistent than, say, Thompson sampling. However, UCB does consistently lead to lower final regrets than $\varepsilon$-Greedy, so if choosing between these two based solely on these plots, I would choose to use UCB. Thompson sampling does, despite its larger variation, lead to lower cumulative regrets overall than both UCB and $\varepsilon$-Greedy. Finally, ETC also seems to find a good balance between these two approaches, but it has a large variation in its greedy phase and exhibits a sudden change in behavior that requires knowledge of the MAB's time horizon. In sum, it is difficult to say which algorithm is "best," and which algorithm we choose to deploy in practice will vary depending on our knowledge of the MAB's parameters as well as our specific goals, specifically, whether we prefer consistent regret accumulation or a lower final regret that may not be as consistent.
